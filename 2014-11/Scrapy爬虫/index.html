<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>互联网商业数据抓取之一.工具篇(Scrapy) - pengming's blog</title>
    <meta name="description" content="jelyll theme">
    <meta name="author" content="hushaw">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/static/favicon.ico" rel="shortcut icon">
    <link href="/static/css/bootstrap.css" rel="stylesheet" type="text/css" media="all">
    <link href="/static/css/font-awesome.css" rel="stylesheet" type="text/css" media="all">
    <link href="/static/google-code-prettify/prettify.css" rel="stylesheet" type="text/css" media="all">
    <link href="/static/css/application.css" rel="stylesheet" type="text/css" media="all">
    <script src="/static/js/jquery.js" type="text/javascript"></script>
    <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
</head>
<body>
     <div class="page-container">
     <div class="page-heading">
         <div class="page-brand">
             <h1><a href="/">心藏一片海</a></h1>
             <h2>因为深情，所以偏执；因为寡言，所以沉默；因为昏聩，所以存留</h2>
         </div>
         <div class="page-navbar">
             <div class="page-navbar-container">
                 <ul class="page-nav">
                     <li><a href="/">Home</a></li>
                     <li><a href="/archive.html">Archive</a></li>
                     <li><a href="/about.html">About</a></li>
                 </ul>
             </div>
         </div>
     </div>
     <div class="page-article">
         <div class="page-content">
    <div class="post-heading">
        互联网商业数据抓取之一.工具篇(Scrapy)
    </div>
    <div class="post-meta">
        <span>
            发布时间:
            <a href="/2014-11/Scrapy%E7%88%AC%E8%99%AB/">2014年11月07日</a>
        </span>
        <span><i class="fa fa-ellipsis-v"></i></span>
        <span>
            分类:
            
            <a href="/categories.html#数据-ref">数据</a>
            
        </span>
        <span><i class="fa fa-ellipsis-v"></i></span>
        <span>
            标签:
            
            <a href="/tags.html#地图，数据，位置，商业-ref">地图，数据，位置，商业</a>
            
        <span>
    </div>
    <div class="post-entry">
        <h2>一.绪言</h2>

<p>工作越深入，越能感觉到数据的力量。有一类数据与位置相关，它存在于热闹大街，存在于高楼大厦，甚至存在于深深民巷。比如路边餐饮类的连锁店，比如宇楼间的品牌服装，比如街角约会的咖啡馆，比如三星四星五星级的酒店。如果你想做煎饼果子的老板，那好，找个人流量大的地铁站两平米足亦，地铁站这条数据就是你选址的原因；如果你联系上了连锁店的加盟商，比如你要当KFC的店长，那好，你选择店址时是不是要看看周边有没有McDonald，甚至有没有山西面馆，沙县小吃；如果你想开家独立书店，区位选择是不是也要符合你的气质？我想你比较心仪繁华静处的一隅，隔街即是欣欣向荣的商业圈落，关上门又仿佛来到了又一个世界。你更愿意跟咖啡馆做邻居而非跟KTV。这一切，都是抉择，抉择的依据是数据。</p>

<p>在这里，我要收集这些数据，整合这些数据，分析这些数据，挖掘这些数据，可视化这些数据。三毛对荷西说：“每想你一次，天上飘落一粒沙，从此形成了撒哈拉！” 数据一样，每爬一条，库里多了一条纪录，不久变形成了数据的海洋。</p>

<h2>二.工具</h2>

<p>Scrapy是python语言的爬虫框架，框架基于事件驱动网络框架Twisted，模块化管理，结构清晰，省去了自定义爬虫书写请求语句的过程，在其官方有这样一张架构图。</p>

<p><img src="http://scrapy-chs.readthedocs.org/zh_CN/latest/_images/scrapy_architecture.png" alt="" /></p>

<blockquote><ol>
<li>引擎打开一个网站(open a domain)，找到处理该网站的Spider并向该spider请求第一个要爬取的URL(s)。</li>
<li>引擎从Spider中获取到第一个要爬取的URL并在调度器(Scheduler)以Request调度。</li>
<li>引擎向调度器请求下一个要爬取的URL。</li>
<li>调度器返回下一个要爬取的URL给引擎，引擎将URL通过下载中间件(请求(request)方向)转发给下载器(Downloader)。</li>
<li>一旦页面下载完毕，下载器生成一个该页面的Response，并将其通过下载中间件(返回(response)方向)发送给引擎。</li>
<li>引擎从下载器中接收到Response并通过Spider中间件(输入方向)发送给Spider处理。</li>
<li>Spider处理Response并返回爬取到的Item及(跟进的)新的Request给引擎。</li>
<li>引擎将(Spider返回的)爬取到的Item给Item Pipeline，将(Spider返回的)Request给调度器。</li>
<li>(从第二步)重复直到调度器中没有更多地request，引擎关闭该网站。</li>
</ol>
</blockquote>

<h2>三.安装</h2>

<p>我系统的版本是Ubuntu 14.04 LTS.</p>

<p>安装时，安装</p>

<blockquote><ul>
<li>pip ( python 包管理工具)</li>
<li><a href="https://pypi.python.org/pypi/service_identity">service_identity </a> ( 与OpenSSL相关)</li>
<li><a href="http://twistedmatrix.com/trac/">Twisted</a></li>
<li>Scrapy(pip install Scrapy)</li>
</ul>
</blockquote>

<p>安装后通过scrapy -version查看版本号，要确认是否为最新版本（截至笔者发稿，<a href="scrapy.org">Scrapy官方</a> 最新版本是Scrapy 0.24.4）</p>

<h2>四.实例</h2>

<p><a href="http://www.subway.com.cn/">赛百味</a> （英语：Subway）是一间起源于美国的跨国快餐连锁店，主要贩售三明治和沙拉，是世界上扩张最快及最大单一品牌连锁店。目前在中国一共493家，<a href="http://www.subway.com.cn/store/shop.php">浏览这里</a> 。下面我用scrapy对所以门店地址信息进行抓取并生成空间数据。</p>

<p>第一步：创建项目
创建一个chainstore的项目，cd进目录，运行命令：</p>

<pre><code>scrapy startproject  chainstore</code></pre>


<p>定义Item，这里要采取赛百味门店的店名，地址，编码，电话。所以，修改items.py文件：</p>

<pre><code>class subWayItem(scrapy.Item):
    subWayName = scrapy.Field()
    subWayAddress = scrapy.Field()
    subWayCode = scrapy.Field()
    subWayPhone = scrapy.Field()
</code></pre>


<p>第二步：编写爬虫
在spiders中新建subway.py，编写爬虫程序。</p>

<pre><code>import scrapy
from chainstore.items import subWayItem
from bs4 import BeautifulSoup
class SubwaySpider(scrapy.Spider):
    name = "subway"
    start_urls = ["http://www.subway.com.cn/store/shop.php"]
    def parse(self,response):
        html = BeautifulSoup(response.body)
        shop = html.find_all("div",class_="shop")
        for sp in shop:
            content = sp.find_all("li")
            sp_name = content[0].find('b').text
            sp_address = content[1].text[:-7]
            sp_code = content[1].text[-5:]
            sp_phone = content[2].text
            item = subWayItem()
            item['subWayName'] = sp_name
            item['subWayAddress'] = sp_address
            item['subWayCode'] = sp_code
            item['subWayPhone'] = sp_phone
            yield item
</code></pre>


<p>官网上对网页的解析是xpath，我比较习惯用<a href="http://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html">Beautiful Soup</a> ,4版本后BS较3改动较大，注意看文档。
进入项目的根目录（scrap.cfg文件对根目录有定义），运行：</p>

<pre><code>scrapy crawl  subway</code></pre>


<p>最简单的数据保存方法是，运行：</p>

<pre><code>scrapy crawl subway -o result.json</code></pre>


<p>截至到目前，我们没有运用pipelines文件。pipelines.py是为了存储提取到的Item。
下面通过编写pipelines文件来进行爬取数据的保存。</p>

<pre><code>import json
import codecs
class JsonWithEncodingPipeline(object):
    def __init__(self):
        self.file = codecs.open('result2.json', 'w', encoding='utf-8')
    def process_item(self, item, spider):
        line = json.dumps(dict(item), ensure_ascii=False) + "\n"
        self.file.write(line)
        return item
    def spider_closed(self, spider):
        self.file.close()</code></pre>


<p>添加一个Item Pipeline组件。
在setting.py中添加</p>

<pre><code>ITEM_PIPELINES =['chainstore.pipelines.JsonWithEncodingPipeline']</code></pre>


<p>执行scrapy crawl  subway后，我们发现，根目录下有个result2.json生成。
总结一句：当Item在Spider中被收集之后，它将会被传递到Item Pipeline，一些组件会按照一定的顺序执行对Item的处理。
爬下来的数据截图：</p>

<p><img src="/static/img/subway.png" alt="" /></p>

<p>第三步：地理编码</p>

<p>数据虽然爬下来了，更艰难的一步是进行空间化。我将目录这样安排：</p>

<p><img src="/static/img/file.png" alt="" /></p>

<p>互联网地图很多提供了地理编码的API接口。这里我们采用百度的地理编码。</p>

<pre><code>from bs4 import BeautifulSoup
class GEOCODING():
    def __init__(self):
        self.headers = {'User-Agent':'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'}
    #百度地理编码
    def geocoding(self,ad):
        str_url = 'http://api.map.baidu.com/geocoder?address='+ ad +'&output=xml&key=XXXXXXXXXXXXXX'
        req = urllib2.Request(url = str_url,headers = self.headers)
        codingRes = urllib2.urlopen(req,timeout = 5).read()
        xmlSoup = BeautifulSoup(codingRes, "xml")
        x = xmlSoup.find('lat').string
        y = xmlSoup.find('lng').string
        return  x,y</code></pre>


<p>此外，需要说明的是地理编码过程从来没有真正能一次能顺利完成的，尽量保证地址按国家省份市县乡镇的顺序写完整 。一次不行，找到原因，try again.</p>

<p>第四步：坐标偏移</p>

<p>谈到这里不得不解释一下为何要进行坐标偏移。</p>

<p>目前国际上通用的空间坐标系统是WGS84坐标，通过GPS采集的坐标点就是这个坐标系统下的真实坐标。</p>

<p>为了国家安全和社会稳定，中国国家测绘局制订的地理信息系统的坐标系统，即“国测局-2002”，江湖上俗称的“火星坐标”，英文是“GCJ-02”。测绘局规定所以的互联网地图行业出版的地图都必须通过该加密算法生成GCJ-02坐标系统，不得使用原始WGS84坐标。</p>

<p>百度和高德这些知名地图厂商，又在GCJ-02坐标系统上融入自己的加密算法，生成了自己的特定坐标系统。也就是进行了二次加密。ps.google地图采用的是高德的数据，即高德的坐标系统。</p>

<p>第三步中我们用的百度的地理编码，得到的经纬度坐标自然也是百度坐标系统下的。所以需要反偏回来，既然反偏回来，首先要从百度坐标系转到GCJ-02,再从GCJ-02转到WGS84.</p>

<pre><code> unBdLng, unBdLat = tranXY.unBdCoor(float(y),float(x))
 wgsLng, wgsLat = tranXY.unShiftCoor(unBdLng, unBdLat)</code></pre>


<p>第五步：生成GeoJson</p>

<p><a href="http://geojson.org/">GeoJson</a> 是一种通用的国际地理格式，使用方便。</p>

<pre><code>import sys
import json
sys.path.append("../geocoding/")
from geocoding import GEOCODING
import codecs
class GEOJSON():
    def __init__(self):
        pass
    def GeoJson(self,Geolist,filename):
        file = codecs.open(filename, 'w')
        GeoJson =  {"type":'FeatureCollection',"features":Geolist}
        GeoJson = json.dumps(dict(GeoJson))
        file.write(GeoJson)
        file.close()</code></pre>


<p>处理好的GeoJson数据如图</p>

<p><img src="/static/img/geojson.png" alt="" /></p>

<p>第六步：简单可视化</p>

<p>如果你安装了qgis，打开qgis，直接加载geojson，可预览，之后想存什么格式存什么格式。</p>

<p><img src="/static/img/qgis.png" alt="" /></p>

    </div>
</div>
<div class="post-blank post-pager">
    <ul class="pager">
        
        <li class="previous"><a href="/2014-10/%E5%8D%81%E6%9C%88%E6%88%91%E5%8E%BB%E5%93%AA%E5%84%BF%E4%BA%86/">&larr; 上一篇</a></li>
        
        
        <li class="next"><a href="/2014-12/%E6%88%91%E7%9A%84%E6%8A%97%E7%97%94%E6%97%A5%E8%AE%B0/">下一篇 &rarr;</a></li>
        
    </ul>
</div>
<!--<div class="page-blank">
    <div id="disqus_thread"></div>
    <script type="text/javascript">
        var disqus_shortname = 'shawhu';
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>
        Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a>
    </noscript>
</div>

-->
<div class="ds-thread"></div>
<script type="text/javascript">
var duoshuoQuery = {short_name:"pengm"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0]
		 || document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>

     </div>
     <div class="page-footer">
         <span>
             Powered by <a href="https://github.com/mojombo/jekyll">Jekyll</a> 
             | Hosted by <a href="http://github.com">Github.com</a> 
             | UI Designed by <a href="/about.html">hushaw</a> 
             | <span class="page-generator-time">2014年12月09日 16时34分24秒 </span> 
             | 本站理念：心藏一片海，无所畏惧..
         </span>
     </div>
     </div>
     <script src="/static/js/bootstrap.js" type="text/javascript"></script>
     <script src="/static/google-code-prettify/prettify.js" type="text/javascript"></script>
     <script type="text/javascript">
        $(document).ready(function() {
            $('pre').addClass('prettyprint').attr('style', 'overflow:auto');
            prettyPrint();
        });
     </script>
     <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-56259167-1', 'auto');
  ga('send', 'pageview');

</script>
</body>
</html>

